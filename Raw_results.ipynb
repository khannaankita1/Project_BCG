{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\ankita.khanna\\.conda\\envs\\pyspark-env\\lib\\site-packages (3.4.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\ankita.khanna\\.conda\\envs\\pyspark-env\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\ankita.khanna\\.conda\\envs\\pyspark-env\\lib\\site-packages (2.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config_path = \"C:/Users/ankita.khanna/Desktop/Project/config.json\"\n",
    "with open(config_path, 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "input_files = config['INPUT_FILENAME']\n",
    "output_format = config['FILE_FORMAT']['Output']\n",
    "output_path = config['OUTPUT_PATH']\n",
    "\n",
    "charges_df = spark.read.csv(input_files['Charges'], header=True, inferSchema=True)\n",
    "damages_df = spark.read.csv(input_files['Damages'], header=True, inferSchema=True)\n",
    "endorse_df = spark.read.csv(input_files['Endorse'], header=True, inferSchema=True)\n",
    "primary_person_df = spark.read.csv(input_files['Primary_Person'], header=True, inferSchema=True)\n",
    "units_df = spark.read.csv(input_files['Units'], header=True, inferSchema=True)\n",
    "restrict_df = spark.read.csv(input_files['Restrict'], header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "def get_output_path(problem_number):\n",
    "    return output_path[problem_number]\n",
    "\n",
    "def write_output(df,problem_number, format):\n",
    "    return df.write.format(format).save(get_output_path(problem_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1 -Find the number of crashes (accidents) in which number of males killed are greater than 2?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df_deaths_per_crash = (\n",
    "    primary_person_df.filter((primary_person_df[\"PRSN_GNDR_ID\"] == 'MALE') &\n",
    "                             (primary_person_df.DEATH_CNT == 1))\n",
    "    .groupBy(\"CRASH_ID\")\n",
    "    .agg(count(\"*\").alias(\"death_per_crash\"))\n",
    ")\n",
    "print(df_deaths_per_crash.filter(df_deaths_per_crash.death_per_crash>2).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2-  How many two wheelers are booked for crashes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, instr, lower\n",
    "\n",
    "df_count_of_two_wheeler_booked = (units_df\n",
    ".filter((instr(lower(units_df.VEH_BODY_STYL_ID), \"motorcycle\") > 0)\n",
    "        | (units_df.UNIT_DESC_ID ==\"PEDALCYCLIST\")\n",
    "))\n",
    "\n",
    "df_count_of_two_wheeler_booked.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3- Determine the Top 5 Vehicle Makes of the cars present in the crashes in which driver died and Airbags did not deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------------------+\n",
      "|  VEH_MAKE_ID|Death_per_vehicle_make_without_airbags|\n",
      "+-------------+--------------------------------------+\n",
      "|       NISSAN|                                     4|\n",
      "|    CHEVROLET|                                     3|\n",
      "|        HONDA|                                     2|\n",
      "|         FORD|                                     2|\n",
      "|     CADILLAC|                                     1|\n",
      "|MERCEDES-BENZ|                                     1|\n",
      "|      PONTIAC|                                     1|\n",
      "|        BUICK|                                     1|\n",
      "|          KIA|                                     1|\n",
      "|     CHRYSLER|                                     1|\n",
      "+-------------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr, lower, count\n",
    "\n",
    "df_top_5_vehicles_without_airbag = (\n",
    "    primary_person_df\n",
    "    .join(units_df, [\"CRASH_ID\",\"UNIT_NBR\"], \"inner\")\n",
    "    .filter(\n",
    "        (instr(lower(units_df.VEH_BODY_STYL_ID), \"car\") > 0) &\n",
    "        (primary_person_df.PRSN_TYPE_ID == 'DRIVER') &\n",
    "        (primary_person_df.DEATH_CNT == 1) &\n",
    "        (primary_person_df.PRSN_AIRBAG_ID == 'NOT DEPLOYED') \n",
    "    )\n",
    "    .groupBy(units_df.VEH_MAKE_ID)\n",
    "    .agg(count(\"*\").alias(\"Death_per_vehicle_make_without_airbags\"))\n",
    "    .orderBy(count(\"*\").alias(\"Death_per_vehicle_make_without_airbags\").desc())\n",
    ")\n",
    "\n",
    "df_top_5_vehicles_without_airbag.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------+\n",
      "|VEH_MAKE_ID|Death_per_vehicle_make_without_airbags|\n",
      "+-----------+--------------------------------------+\n",
      "|     NISSAN|                                     4|\n",
      "|  CHEVROLET|                                     3|\n",
      "|      HONDA|                                     2|\n",
      "|       FORD|                                     2|\n",
      "|   CADILLAC|                                     1|\n",
      "+-----------+--------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr, lower, count, dense_rank, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window=Window.orderBy(col(\"Death_per_vehicle_make_without_airbags\").desc())\n",
    "\n",
    "df_top_5_vehicles_without_airbag = (\n",
    "    primary_person_df\n",
    "    .join(units_df, [\"CRASH_ID\",\"UNIT_NBR\"], \"inner\")\n",
    "    .filter(\n",
    "        (instr(lower(units_df.VEH_BODY_STYL_ID), \"car\") > 0) &\n",
    "        (primary_person_df.PRSN_TYPE_ID == 'DRIVER') &\n",
    "        (primary_person_df.DEATH_CNT == 1) &\n",
    "        (primary_person_df.PRSN_AIRBAG_ID == 'NOT DEPLOYED') \n",
    "    )\n",
    "    .groupBy(units_df.VEH_MAKE_ID)\n",
    "    .agg(count(\"*\").alias(\"Death_per_vehicle_make_without_airbags\"))\n",
    "    .withColumn(\"rank\", dense_rank().over(window))\n",
    "    .filter(col(\"rank\") < 6)\n",
    "    .drop(\"rank\")\n",
    "    )\n",
    "\n",
    "df_top_5_vehicles_without_airbag.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4 -Determine number of Vehicles with driver having valid licences involved in hit and run? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450\n"
     ]
    }
   ],
   "source": [
    "df_driver_without_driving_license = (\n",
    "    primary_person_df\n",
    "    .join(units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "    .filter(\n",
    "        (primary_person_df.PRSN_TYPE_ID == 'DRIVER') &\n",
    "        (units_df.VEH_HNR_FL == 'Y') &\n",
    "        (\n",
    "            (primary_person_df.DRVR_LIC_TYPE_ID.isin(\"DRIVER LICENSE\", \"COMMERCIAL DRIVER LIC\", \"OCCUPATIONAL\")) &\n",
    "            (~primary_person_df.DRVR_LIC_CLS_ID.isin(\"UNLICENSED\", \"NA\", \"UNKNOWN\"))\n",
    "        )\n",
    "    )\n",
    "    .select(\"CRASH_ID\", \"UNIT_NBR\")\n",
    "    .distinct()\n",
    "    .count()\n",
    ")\n",
    "\n",
    "print(df_driver_without_driving_license)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem -5 -- Which state has highest number of accidents in which females are not involved? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------------------+\n",
      "|DRVR_LIC_STATE_ID|Non Women Driver Accident Cases|\n",
      "+-----------------+-------------------------------+\n",
      "|            Texas|                          61022|\n",
      "+-----------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, dense_rank, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.orderBy(col(\"Non Women Driver Accident Cases\").desc())\n",
    "\n",
    "df_state_having_highest_accidents_without_women = (\n",
    "    primary_person_df\n",
    "    .filter((primary_person_df.PRSN_GNDR_ID != 'FEMALE')\n",
    "            &(primary_person_df.PRSN_TYPE_ID == 'DRIVER'))\n",
    "    .groupBy(primary_person_df.DRVR_LIC_STATE_ID)\n",
    "    .agg(countDistinct(\"CRASH_ID\").alias(\"Non Women Driver Accident Cases\"))\n",
    "    .withColumn(\"rank\", dense_rank().over(window))\n",
    "\t.filter(col(\"rank\") == 1)\n",
    "\t.drop(\"rank\")\n",
    ")\n",
    "\n",
    "\n",
    "df_state_having_highest_accidents_without_women.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem -6 Which are the Top 3rd to 5th VEH_MAKE_IDs that contribute to a largest number of injuries including death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------------+----------------+\n",
      "|VEH_MAKE_ID|Total_Injuries|Total_Deaths|Total_Casualties|\n",
      "+-----------+--------------+------------+----------------+\n",
      "|     TOYOTA|          3335|          13|            3348|\n",
      "|      DODGE|          2469|           8|            2477|\n",
      "|     NISSAN|          2428|          13|            2441|\n",
      "+-----------+--------------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import sum, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.orderBy(col(\"Total_Casualties\").desc())\n",
    "df_top_vehicle_cat = (\n",
    "    primary_person_df\n",
    "    .join(units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "    .filter(units_df.VEH_MAKE_ID != 'NA')\n",
    "    .groupBy(units_df.VEH_MAKE_ID)\n",
    "    .agg(sum(primary_person_df.TOT_INJRY_CNT).alias(\"Total_Injuries\"), \n",
    "         sum(primary_person_df.DEATH_CNT).alias(\"Total_Deaths\"))\n",
    "    .withColumn(\"Total_Casualties\", col(\"Total_Injuries\") + col(\"Total_Deaths\"))\n",
    "    .withColumn(\"rank\", dense_rank().over(window))\n",
    "\t.filter((col(\"rank\") <6 ) & (col(\"rank\")>2))\n",
    "\t.drop(\"rank\")\n",
    ")\n",
    "\n",
    "df_top_vehicle_cat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem -7  For all the body styles involved in crashes, mention the top ethnic user group of each unique body style  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|    VEH_BODY_STYL_ID|PRSN_ETHNICITY_ID|\n",
      "+--------------------+-----------------+\n",
      "|           AMBULANCE|            WHITE|\n",
      "|                 BUS|            BLACK|\n",
      "|      FARM EQUIPMENT|            WHITE|\n",
      "|          FIRE TRUCK|            WHITE|\n",
      "|          MOTORCYCLE|            WHITE|\n",
      "|NEV-NEIGHBORHOOD ...|            WHITE|\n",
      "|PASSENGER CAR, 2-...|            WHITE|\n",
      "|PASSENGER CAR, 4-...|            WHITE|\n",
      "|              PICKUP|            WHITE|\n",
      "|    POLICE CAR/TRUCK|            WHITE|\n",
      "|   POLICE MOTORCYCLE|            WHITE|\n",
      "|SPORT UTILITY VEH...|            WHITE|\n",
      "|               TRUCK|            WHITE|\n",
      "|       TRUCK TRACTOR|            WHITE|\n",
      "|                 VAN|            WHITE|\n",
      "|   YELLOW SCHOOL BUS|            BLACK|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window= Window.partitionBy(\"VEH_BODY_STYL_ID\").orderBy(col(\"Count_of_crashes\").desc())\n",
    "df_count_of_crashes = (primary_person_df.join(units_df, [\"CRASH_ID\",\"UNIT_NBR\"], \"inner\")\n",
    "            .filter((~units_df.VEH_BODY_STYL_ID.isin([\"NA\", \"UNKNOWN\", \"NOT REPORTED\", \"OTHER  (EXPLAIN IN NARRATIVE)\"]))\n",
    "            &(~primary_person_df.PRSN_ETHNICITY_ID.isin([\"NA\", \"UNKNOWN\"])))\n",
    "            .groupby(\"VEH_BODY_STYL_ID\", \"PRSN_ETHNICITY_ID\")\n",
    "            .agg(countDistinct(\"CRASH_ID\").alias(\"Count_of_crashes\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") == 1)\n",
    "            .drop(\"rank\", \"Count_of_crashes\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem-8  : Among the crashed cars, what are the Top 5 Zip Codes with highest number crashes with alcohols as the contributing factor to a crash (Use Driver Zip Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|DRVR_ZIP|Count_of_crashes|\n",
      "+--------+----------------+\n",
      "|   76010|              52|\n",
      "|   78521|              47|\n",
      "|   78130|              41|\n",
      "|   75067|              41|\n",
      "|   78542|              38|\n",
      "|   78550|              35|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, instr, lower, col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.orderBy(col(\"Count_of_crashes\").desc())\n",
    "\n",
    "df_drunk_and_drive_cases = (\n",
    "    primary_person_df.join(units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "    .filter(\n",
    "        ((instr(lower(units_df.CONTRIB_FACTR_1_ID), \"alcohol\") > 0) |\n",
    "        (instr(lower(units_df.CONTRIB_FACTR_P1_ID), \"alcohol\") > 0) |\n",
    "        (instr(lower(units_df.CONTRIB_FACTR_2_ID), \"alcohol\") > 0))\n",
    "        & (primary_person_df.DRVR_ZIP !='NULL')\n",
    "    )\n",
    "    .groupby(\"DRVR_ZIP\")\n",
    "    .agg(countDistinct(\"CRASH_ID\").alias(\"Count_of_crashes\"))\n",
    "    .withColumn(\"rank\", dense_rank().over(window))\n",
    "    .filter(col(\"rank\") < 6)\n",
    "    .drop(\"rank\")\n",
    ")\n",
    "\n",
    "df_drunk_and_drive_cases.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 9 - Count of Distinct Crash IDs where No Damaged Property was observed and Damage Level (VEH_DMAG_SCL~) is above 4 and car avails Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Count_of_crashes|\n",
      "+----------------+\n",
      "|               8|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_no_damage_property = (\n",
    "units_df.join(damages_df, \"CRASH_ID\", \"inner\")\n",
    ".filter((((units_df.VEH_DMAG_SCL_1_ID).isin(\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\")) |\n",
    "((units_df.VEH_DMAG_SCL_2_ID).isin(\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\"))) &\n",
    "(damages_df.DAMAGED_PROPERTY == \"NONE\") &\n",
    "(units_df.FIN_RESP_TYPE_ID.isin(\"LIABILITY INSURANCE POLICY\", \"PROOF OF LIABILITY INSURANCE\")))\n",
    ".agg(countDistinct(\"CRASH_ID\").alias(\"Count_of_crashes\"))\n",
    "                         )\n",
    "\n",
    "df_no_damage_property.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 10  Determine the Top 5 Vehicle Makes where drivers are charged with speeding related offences, has licensed Drivers, used top 10 used vehicle colours and has car licensed with the Top 25 states with highest number of offences (to be deduced from the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Texas', 'Mexico', 'Louisiana', 'New Mexico', 'California', 'Florida', 'Oklahoma', 'Arkansas', 'Arizona', 'Georgia', 'Illinois', 'Colorado', 'Mississippi', 'Missouri', 'Tennessee', 'North Carolina', 'Kansas', 'Alabama', 'Ohio', 'Michigan', 'New York', 'Washington', 'Virginia', 'Nevada', 'Indiana', 'South Carolina', 'Pennsylvania']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "#  top 25 list of citites having highest number of offences\n",
    "window = Window.orderBy(col(\"Count_of_crashes_having_charges\").desc())\n",
    "\n",
    "top_25_states_df = (\n",
    "    primary_person_df.join(charges_df, [\"CRASH_ID\", \"PRSN_NBR\", \"UNIT_NBR\"], \"inner\")\n",
    "    .filter(~primary_person_df.DRVR_LIC_STATE_ID.isin(\"Unknown\", \"NA\", \"Other\"))\n",
    "    .groupBy(\"DRVR_LIC_STATE_ID\")\n",
    "    .agg(countDistinct(charges_df.CRASH_ID).alias(\"Count_of_crashes_having_charges\"))\n",
    "    .withColumn(\"rank\", dense_rank().over(window))\n",
    "    .filter(col(\"rank\") < 26)\n",
    "    .drop(\"rank\", \"Count_of_crashes_having_charges\")\n",
    ")\n",
    "\n",
    "list_of_top_states = list(row.DRVR_LIC_STATE_ID for row in top_25_states_df.collect())\n",
    "print(list_of_top_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['WHI', 'BLK', 'SIL', 'GRY', 'BLU', 'RED', 'GRN', 'MAR', 'TAN', 'GLD']\n"
     ]
    }
   ],
   "source": [
    "#used top 10 used vehicle colours with highest number of offences\n",
    "window = Window.orderBy(col(\"Count_of_crashes_having_charges\").desc())\n",
    "top_10_colours = (\n",
    "    units_df.join(charges_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "    .filter(~units_df.VEH_COLOR_ID.isin(\"NA\"))\n",
    "    .groupBy(\"VEH_COLOR_ID\")\n",
    "    .agg(countDistinct(charges_df.CRASH_ID).alias(\"Count_of_crashes_having_charges\"))\n",
    "    .withColumn(\"rank\", dense_rank().over(window))\n",
    "    .filter(col(\"rank\") < 11)\n",
    "    .drop(\"rank\", \"Count_of_crashes_having_charges\")\n",
    ")\n",
    "list_of_top_colours = list(row.VEH_COLOR_ID for row in top_10_colours.collect())\n",
    "print(list_of_top_colours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|VEH_MAKE_ID|\n",
      "+-----------+\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.orderBy(col(\"Count_of_crashes_having_charges\").desc())\n",
    "\n",
    "df_top_5_makers=(\n",
    "    primary_person_df.join(charges_df, [\"CRASH_ID\", \"PRSN_NBR\", \"UNIT_NBR\"], \"inner\")\n",
    "    .join(units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "    .filter((\n",
    "            (primary_person_df.DRVR_LIC_TYPE_ID.isin(\"DRIVER LICENSE\", \"COMMERCIAL DRIVER LIC\", \"OCCUPATIONAL\")) |\n",
    "            (~primary_person_df.DRVR_LIC_CLS_ID.isin(\"UNLICENSED\", \"NA\", \"UNKNOWN\")))&\n",
    "            (instr(lower(charges_df.CHARGE),\"speed\") > 0) &\n",
    "            (units_df.VEH_COLOR_ID.isin(list_of_top_colours)) &\n",
    "            (units_df.VEH_LIC_STATE_ID.isin(list_of_top_states)))\n",
    "    .groupby(\"VEH_MAKE_ID\")\n",
    "    .agg(countDistinct(charges_df.CRASH_ID).alias(\"Count_of_crashes_having_charges\"))\n",
    "    .withColumn(\"rank\", dense_rank().over(window))\n",
    "    .filter(col(\"rank\") < 6)\n",
    "    .drop(\"rank\", \"Count_of_crashes_having_charges\")\n",
    "    ).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
