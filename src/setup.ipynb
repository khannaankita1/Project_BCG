class DataProcessor:
    import json
    from pyspark.sql import SparkSession
    from pyspark.sql import SparkSession, DataFrame
    from pyspark.sql.functions import count, instr, lower, dense_rank, col, countDistinct, sum
    from pyspark.sql.window import Window
    

    def __init__(self, config_path):
        with open(config_path, 'r') as config_file:
            self.config = json.load(config_file)
     
    def read_input_files(self):
        input_files = self.config['INPUT_FILENAME']
        self.charges_df = self.spark.read.csv(input_files['Charges'], header=True, inferSchema=True)
        self.damages_df = self.spark.read.csv(input_files['Damages'], header=True, inferSchema=True)
        self.endorse_df = self.spark.read.csv(input_files['Endorse'], header=True, inferSchema=True)
        self.primary_person_df = self.spark.read.csv(input_files['Primary_Person'], header=True, inferSchema=True)
        self.units_df = self.spark.read.csv(input_files['Units'], header=True, inferSchema=True)
        self.restrict_df = self.spark.read.csv(input_files['Restrict'], header=True, inferSchema=True)
        
    def get_output_path(self, problem_number):
        output_path = self.config['OUTPUT_PATH']
        return output_path[problem_number]
    
    def write_output(self, df, problem_number, format):
        output_format = self.config['FILE_FORMAT']['Output']
        df.write.format(format).save(self.get_output_path(problem_number))

config_path = "C:/Users/ankita.khanna/Desktop/Project/config.json"
data_processor = DataProcessor(config_path)
data_processor.read_input_files()

