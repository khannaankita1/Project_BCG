{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Dynamic\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing setup file for read and write transformations of input and output data from config file\n",
    "from src.setup import  DataProcessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 1 -  Find the number of crashes (accidents) in which number of males killed are greater than 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "class CrashAnalysis:\n",
    "    def __init__(self, primary_person_df: DataFrame):\n",
    "        self.primary_person_df = primary_person_df\n",
    "\n",
    "    def filter_data(self, gender: str = 'MALE') -> DataFrame:\n",
    "        df = self.primary_person_df.filter(\n",
    "            (self.primary_person_df[\"PRSN_GNDR_ID\"] == gender) &\n",
    "            (self.primary_person_df.DEATH_CNT == 1) \n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def count_per_crash(self, filtered_df: DataFrame) -> DataFrame:\n",
    "        return filtered_df.groupBy(\"CRASH_ID\").agg(count(\"*\").alias(\"death_per_crash\"))\n",
    "\n",
    "    def filter_by_threshold(self, count_df: DataFrame, threshold: int) -> DataFrame:\n",
    "        return count_df.filter(count_df.death_per_crash > threshold)\n",
    "\n",
    "    def get_count(self, filtered_count_df: DataFrame) -> int:\n",
    "        return filtered_count_df.count()\n",
    "\n",
    "    def run_analysis(self, gender: str = 'MALE', threshold: int = 2) -> int:\n",
    "        filtered_df = self.filter_data(gender)\n",
    "        count_df = self.count_per_crash(filtered_df)\n",
    "        filtered_count_df = self.filter_by_threshold(count_df, threshold)\n",
    "        return self.get_count(filtered_count_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 2 - How many two wheelers are booked for crashes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr, lower\n",
    "\n",
    "class TwoWheelerCounter:\n",
    "    def __init__(self, units_df):\n",
    "        self.units_df = units_df\n",
    "        \n",
    "    def two_wheelers(self):\n",
    "        return self.units_df.filter(\n",
    "            (instr(lower(self.units_df.VEH_BODY_STYL_ID), \"motorcycle\") > 0) |\n",
    "            (self.units_df.UNIT_DESC_ID == \"PEDALCYCLIST\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 3- Determine the Top 5 Vehicle Makes of the cars present in the crashes in which driver died and Airbags did not deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------+\n",
      "|VEH_MAKE_ID|Death_per_vehicle_make_without_airbags|\n",
      "+-----------+--------------------------------------+\n",
      "|     NISSAN|                                     4|\n",
      "|  CHEVROLET|                                     3|\n",
      "|      HONDA|                                     2|\n",
      "|       FORD|                                     2|\n",
      "|   CADILLAC|                                     1|\n",
      "+-----------+--------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import instr, lower, count, dense_rank, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class VehicleAirbagAnalysis:\n",
    "    def __init__(self, primary_person_df: DataFrame, units_df: DataFrame):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "\n",
    "    def filter_and_aggregate(self, body_style_keyword: str, person_type: str, death_count: int, airbag_status: str, rank_threshold: int = 5) -> DataFrame:\n",
    "        window = Window.orderBy(col(\"Death_per_vehicle_make_without_airbags\").desc())\n",
    "        \n",
    "        return (\n",
    "            self.primary_person_df\n",
    "            .join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                (instr(lower(self.units_df.VEH_BODY_STYL_ID), body_style_keyword) > 0) &\n",
    "                (self.primary_person_df.PRSN_TYPE_ID == person_type) &\n",
    "                (self.primary_person_df.DEATH_CNT == death_count) &\n",
    "                (self.primary_person_df.PRSN_AIRBAG_ID == airbag_status)\n",
    "            )\n",
    "            .groupBy(self.units_df.VEH_MAKE_ID)\n",
    "            .agg(count(\"*\").alias(\"Death_per_vehicle_make_without_airbags\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") < rank_threshold + 1)\n",
    "            .drop(\"rank\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 4 - Determine number of Vehicles with driver having valid licences involved in hit and run? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class DriverLicenseAnalysis:\n",
    "    def __init__(self, primary_person_df: DataFrame, units_df: DataFrame):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "\n",
    "    def filter_and_count(self, person_type: str, veh_honor_flag: str, valid_lic_types: list, invalid_lic_classes: list) -> int:\n",
    "        return (\n",
    "            self.primary_person_df\n",
    "            .join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                (self.primary_person_df.PRSN_TYPE_ID == person_type) &\n",
    "                (self.units_df.VEH_HNR_FL == veh_honor_flag) &\n",
    "                (\n",
    "                    self.primary_person_df.DRVR_LIC_TYPE_ID.isin(valid_lic_types) &\n",
    "                    (~self.primary_person_df.DRVR_LIC_CLS_ID.isin(invalid_lic_classes))\n",
    "                )\n",
    "            )\n",
    "            .select(\"CRASH_ID\", \"UNIT_NBR\")\n",
    "            .distinct()\n",
    "            .count()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 5 - Which state has highest number of accidents in which females are not involved? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import countDistinct, dense_rank, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class AccidentAnalysis:\n",
    "    def __init__(self, primary_person_df: DataFrame):\n",
    "        self.primary_person_df = primary_person_df\n",
    "\n",
    "    def filter_and_aggregate(self, gender_exclusion: str, person_type: str, group_by_column: str, rank_column: str, rank_value: int = 1) -> DataFrame:\n",
    "        window = Window.orderBy(col(rank_column).desc())\n",
    "        \n",
    "        return (\n",
    "            self.primary_person_df\n",
    "            .filter((self.primary_person_df.PRSN_GNDR_ID != gender_exclusion) &\n",
    "                    (self.primary_person_df.PRSN_TYPE_ID == person_type))\n",
    "            .groupBy(group_by_column)\n",
    "            .agg(countDistinct(\"CRASH_ID\").alias(rank_column))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") == rank_value)\n",
    "            .drop(\"rank\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 6 Which are the Top 3rd to 5th VEH_MAKE_IDs that contribute to a largest number of injuries including death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class TopVehicleCategories:\n",
    "    def __init__(self, primary_person_df, units_df):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "        \n",
    "    def get_top_vehicle_categories(self, rank_lower, rank_upper):\n",
    "        window = Window.orderBy(col(\"Total_Casualties\").desc())\n",
    "        \n",
    "        df_top_vehicle_cat = (\n",
    "            self.primary_person_df\n",
    "            .join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(self.units_df.VEH_MAKE_ID != 'NA')\n",
    "            .groupBy(self.units_df.VEH_MAKE_ID)\n",
    "            .agg(sum(self.primary_person_df.TOT_INJRY_CNT).alias(\"Total_Injuries\"), \n",
    "                 sum(self.primary_person_df.DEATH_CNT).alias(\"Total_Deaths\"))\n",
    "            .withColumn(\"Total_Casualties\", col(\"Total_Injuries\") + col(\"Total_Deaths\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter((col(\"rank\") >= rank_lower) & (col(\"rank\") <= rank_upper))\n",
    "            .drop(\"rank\")\n",
    "        )\n",
    "        \n",
    "        return df_top_vehicle_cat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 7 - For all the body styles involved in crashes, mention the top ethnic user group of each unique body style  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class CountCrashes:\n",
    "    def __init__(self, primary_person_df, units_df):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "        \n",
    "    def count_crashes(self, rank):\n",
    "        window = Window.partitionBy(\"VEH_BODY_STYL_ID\").orderBy(col(\"Count_of_crashes\").desc())\n",
    "        \n",
    "        df_count_of_crashes = (\n",
    "            self.primary_person_df.join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                (~self.units_df.VEH_BODY_STYL_ID.isin([\"NA\", \"UNKNOWN\", \"NOT REPORTED\", \"OTHER  (EXPLAIN IN NARRATIVE)\"])) &\n",
    "                (~self.primary_person_df.PRSN_ETHNICITY_ID.isin([\"NA\", \"UNKNOWN\"]))\n",
    "            )\n",
    "            .groupby(\"VEH_BODY_STYL_ID\", \"PRSN_ETHNICITY_ID\")\n",
    "            .agg(countDistinct(\"CRASH_ID\").alias(\"Count_of_crashes\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") == rank)\n",
    "            .drop(\"rank\", \"Count_of_crashes\")\n",
    "        )\n",
    "        \n",
    "        return df_count_of_crashes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 8 Among the crashed cars, what are the Top 5 Zip Codes with highest number crashes with alcohols as the contributing factor to a crash (Use Driver Zip Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, instr, lower, col, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class DrunkAndDriveCases:\n",
    "    def __init__(self, primary_person_df, units_df, zip_code_col=\"DRVR_ZIP\", rank_lower=1, rank_upper=5):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "        self.zip_code_col = zip_code_col\n",
    "        self.rank_lower = rank_lower\n",
    "        self.rank_upper = rank_upper\n",
    "        \n",
    "    def calculate_drunk_and_drive_cases(self):\n",
    "        window = Window.orderBy(col(\"Count_of_crashes\").desc())\n",
    "        \n",
    "        alcohol_condition = (\n",
    "            (instr(lower(self.units_df.CONTRIB_FACTR_1_ID), \"alcohol\") > 0) |\n",
    "            (instr(lower(self.units_df.CONTRIB_FACTR_P1_ID), \"alcohol\") > 0) |\n",
    "            (instr(lower(self.units_df.CONTRIB_FACTR_2_ID), \"alcohol\") > 0)\n",
    "        )\n",
    "        \n",
    "        df_drunk_and_drive_cases = (\n",
    "            self.primary_person_df.join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                alcohol_condition &\n",
    "                (self.primary_person_df[self.zip_code_col] != 'NULL')\n",
    "            )\n",
    "            .groupby(self.zip_code_col)\n",
    "            .agg(countDistinct(\"CRASH_ID\").alias(\"Count_of_crashes\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\").between(self.rank_lower, self.rank_upper))\n",
    "            .drop(\"rank\")\n",
    "        )\n",
    "        \n",
    "        return df_drunk_and_drive_cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 9 Count of Distinct Crash IDs where No Damaged Property was observed and Damage Level (VEH_DMAG_SCL~) is above 4 and car avails Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, col\n",
    "\n",
    "class NoDamagePropertyCrashes:\n",
    "    def __init__(self, units_df, damages_df):\n",
    "        self.units_df = units_df\n",
    "        self.damages_df = damages_df\n",
    "        \n",
    "    def calculate_no_damage_property_crashes(self):\n",
    "        df_no_damage_property = (\n",
    "            self.units_df.join(damages_df, \"CRASH_ID\", \"inner\")\n",
    "            .filter(\n",
    "                (\n",
    "                    ((self.units_df.VEH_DMAG_SCL_1_ID).isin(\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\")) |\n",
    "                    ((self.units_df.VEH_DMAG_SCL_2_ID).isin(\"DAMAGED 5\", \"DAMAGED 6\", \"DAMAGED 7 HIGHEST\"))\n",
    "                ) &\n",
    "                (self.damages_df.DAMAGED_PROPERTY == \"NONE\") &\n",
    "                (self.units_df.FIN_RESP_TYPE_ID.isin(\"LIABILITY INSURANCE POLICY\", \"PROOF OF LIABILITY INSURANCE\"))\n",
    "            )\n",
    "            .agg(countDistinct(\"CRASH_ID\").alias(\"Count_of_crashes\"))\n",
    "        )\n",
    "        \n",
    "        return df_no_damage_property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 10 -Determine the Top 5 Vehicle Makes where drivers are charged with speeding related offences, has licensed Drivers, used top 10 used vehicle colours and has car licensed with the Top 25 states with highest number of offences (to be deduced from the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, col, dense_rank, instr, lower\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "class TopOffenses:\n",
    "    def __init__(self, primary_person_df, units_df, charges_df):\n",
    "        self.primary_person_df = primary_person_df\n",
    "        self.units_df = units_df\n",
    "        self.charges_df = charges_df\n",
    "    \n",
    "    def top_states(self, limit=25):\n",
    "        window = Window.orderBy(col(\"Count_of_crashes_having_charges\").desc())\n",
    "        top_states_df = (\n",
    "            self.primary_person_df.join(self.charges_df, [\"CRASH_ID\", \"PRSN_NBR\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(~self.primary_person_df.DRVR_LIC_STATE_ID.isin(\"Unknown\", \"NA\", \"Other\"))\n",
    "            .groupBy(\"DRVR_LIC_STATE_ID\")\n",
    "            .agg(countDistinct(self.charges_df.CRASH_ID).alias(\"Count_of_crashes_having_charges\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") <= limit)\n",
    "            .drop(\"rank\", \"Count_of_crashes_having_charges\")\n",
    "        )\n",
    "        return [row.DRVR_LIC_STATE_ID for row in top_states_df.collect()]\n",
    "    \n",
    "    def top_colors(self, limit=10):\n",
    "        window = Window.orderBy(col(\"Count_of_crashes_having_charges\").desc())\n",
    "        top_colors_df = (\n",
    "            self.units_df.join(self.charges_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(~self.units_df.VEH_COLOR_ID.isin(\"NA\"))\n",
    "            .groupBy(\"VEH_COLOR_ID\")\n",
    "            .agg(countDistinct(self.charges_df.CRASH_ID).alias(\"Count_of_crashes_having_charges\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") <= limit)\n",
    "            .drop(\"rank\", \"Count_of_crashes_having_charges\")\n",
    "        )\n",
    "        return [row.VEH_COLOR_ID for row in top_colors_df.collect()]\n",
    "    \n",
    "    def top_vehicle_makers(self, states, colors, limit=5):\n",
    "        window = Window.orderBy(col(\"Count_of_crashes_having_charges\").desc())\n",
    "        top_makers_df = (\n",
    "            self.primary_person_df.join(self.charges_df, [\"CRASH_ID\", \"PRSN_NBR\", \"UNIT_NBR\"], \"inner\")\n",
    "            .join(self.units_df, [\"CRASH_ID\", \"UNIT_NBR\"], \"inner\")\n",
    "            .filter(\n",
    "                (\n",
    "                    (self.primary_person_df.DRVR_LIC_TYPE_ID.isin(\"DRIVER LICENSE\", \"COMMERCIAL DRIVER LIC\", \"OCCUPATIONAL\")) |\n",
    "                    (~self.primary_person_df.DRVR_LIC_CLS_ID.isin(\"UNLICENSED\", \"NA\", \"UNKNOWN\"))\n",
    "                ) &\n",
    "                (instr(lower(self.charges_df.CHARGE), \"speed\") > 0) &\n",
    "                (self.units_df.VEH_COLOR_ID.isin(colors)) &\n",
    "                (self.units_df.VEH_LIC_STATE_ID.isin(states))\n",
    "            )\n",
    "            .groupBy(\"VEH_MAKE_ID\")\n",
    "            .agg(countDistinct(self.charges_df.CRASH_ID).alias(\"Count_of_crashes_having_charges\"))\n",
    "            .withColumn(\"rank\", dense_rank().over(window))\n",
    "            .filter(col(\"rank\") <= limit)\n",
    "            .drop(\"rank\", \"Count_of_crashes_having_charges\")\n",
    "        )\n",
    "        return top_makers_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling main function and instanciating corresponding classes for problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    " \n",
    "    #Instanciating DataProcessor class\n",
    "    data_setup=DataProcessor()\n",
    "\n",
    "    # Problem 1. Find the number of crashes (accidents) in which number of males killed are greater than 2?\n",
    "    analysis = CrashAnalysis(primary_person_df)\n",
    "    result_DF = analysis.run_analysis(gender='MALE', threshold=2)\n",
    "    result_DF.show()\n",
    "    write_output(result_DF,'1',output_format)\n",
    "\n",
    "    # 2. How many two-wheelers are booked for crashes?\n",
    "    two_wheeler_counter = TwoWheelerCounter(units_df)\n",
    "    Result_df = two_wheelers()\n",
    "    print(\"Count of two-wheelers:\", Result_df.count())\n",
    "    write_output(Result_df,'2',output_format)\n",
    "\n",
    "    # 3. Determine the Top 5 Vehicles made of the cars present in the crashes in which a driver died and Airbags did\n",
    "    # not deploy.\n",
    "    analysis = VehicleAirbagAnalysis(primary_person_df, units_df)\n",
    "    df_top_vehicles_without_airbag = analysis.filter_and_aggregate(\n",
    "    body_style_keyword=\"car\", \n",
    "    person_type=\"DRIVER\", \n",
    "    death_count=1, \n",
    "    airbag_status=\"NOT DEPLOYED\",\n",
    "    rank_threshold=5\n",
    "    )\n",
    "    df_top_vehicles_without_airbag.show()\n",
    "    write_output(df_top_vehicles_without_airbag,'3',output_format)\n",
    "\n",
    "    # 4. Determine the number of Vehicles with a driver having valid licences involved in hit-and-run?\n",
    "\n",
    "    analysis = DriverLicenseAnalysis(primary_person_df, units_df)\n",
    "    df_driver_without_driving_license = analysis.filter_and_count(\n",
    "    person_type=\"DRIVER\",\n",
    "    veh_honor_flag=\"Y\",\n",
    "    valid_lic_types=[\"DRIVER LICENSE\", \"COMMERCIAL DRIVER LIC\", \"OCCUPATIONAL\"],\n",
    "    invalid_lic_classes=[\"UNLICENSED\", \"NA\", \"UNKNOWN\"])\n",
    "    print(df_driver_without_driving_license)\n",
    "    write_output(df_top_vehicles_without_airbag,'4',output_format)\n",
    "\n",
    "    # 5. Which state has the highest number of accidents in which females are not involved?\n",
    "\n",
    "    analysis = AccidentAnalysis(primary_person_df)\n",
    "    df_states_having_highest_accidents_without_women = analysis.filter_and_aggregate(\n",
    "    gender_exclusion=\"FEMALE\",\n",
    "    person_type=\"DRIVER\",\n",
    "    group_by_column=\"DRVR_LIC_STATE_ID\",\n",
    "    rank_column=\"Non Women Driver Accident Cases\",\n",
    "    rank_value=1)\n",
    "    df_states_having_highest_accidents_without_women.show()\n",
    "    write_output(df_states_having_highest_accidents_without_women,'5',output_format)\n",
    "\n",
    "\n",
    "    # 6. Which are the Top 3rd to 5th VEH_MAKE_IDs that contribute to a largest number of injuries including death\n",
    "\n",
    "    top_vehicle_categories = TopVehicleCategories(primary_person_df, units_df)\n",
    "    rank_lower = 3\n",
    "    rank_upper = 5\n",
    "    df_top_vehicle_cat = top_vehicle_categories.get_top_vehicle_categories(rank_lower, rank_upper)\n",
    "    df_top_vehicle_cat.show()\n",
    "    write_output(df_top_vehicle_cat,'6',output_format)\n",
    "\n",
    "\n",
    "\n",
    "    # 7. For all the body styles involved in crashes, mention the top ethnic user group of each unique body style\n",
    "    \n",
    "    count_crashes = CountCrashes(primary_person_df, units_df)\n",
    "    df_count_of_crashes = count_crashes.count_crashes(1)\n",
    "    df_count_of_crashes.show()\n",
    "    write_output(df_count_of_crashes,'7',output_format)\n",
    "\n",
    "    # 8. Among the crashed cars, what are the Top 5 Zip Codes with the highest number of crashes with alcohol as the\n",
    "    # contributing factor to a crash (Use Driver Zip Code)\n",
    "    \n",
    "    drunk_and_drive_cases = DrunkAndDriveCases(primary_person_df, units_df, zip_code_col=\"DRVR_ZIP\", rank_lower=1, rank_upper=5)\n",
    "    result_df = drunk_and_drive_cases.calculate_drunk_and_drive_cases()\n",
    "    result_df.show()\n",
    "    write_output(result_df,'8',output_format)\n",
    "\n",
    "\n",
    "\n",
    "    # 9. Count of Distinct Crash IDs where No Damaged Property was observed and Damage Level (VEH_DMAG_SCL~) is above\n",
    "    # 4 and car avails Insurance\n",
    "    no_damage_property_crashes = NoDamagePropertyCrashes(units_df, damages_df)\n",
    "    result_df = no_damage_property_crashes.calculate_no_damage_property_crashes()\n",
    "    result_df.show()\n",
    "    write_output(result_df,'9',output_format)\n",
    "\n",
    "\n",
    "    # 10. Determine the Top 5 Vehicle Makes where drivers are charged with speeding related offences, has licensed\n",
    "    # Drivers, used top 10 used vehicle colours and has car licensed with the Top 25 states with highest number of\n",
    "    # offenses (to be deduced from the data)\n",
    "    \n",
    "    top_offenses = TopOffenses(primary_person_df, units_df, charges_df)\n",
    "    top_states = top_offenses.top_states()\n",
    "    top_colors = top_offenses.top_colors()\n",
    "    top_makers = top_offenses.top_vehicle_makers(top_states, top_colors)\n",
    "    print(\"Top vehicle makers with offenses:\")\n",
    "    top_makers.show()\n",
    "    write_output(top_makers,'9',output_format)\n",
    "\n",
    "\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
